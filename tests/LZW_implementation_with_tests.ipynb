{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "mmZHM1wtffn4",
        "1xCgIMdnIFn2",
        "j1DNUBiQFIEK",
        "DmRmH2ByF57b",
        "W0AHGnxlDQKE"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Implementação do LZW e Trie Compacta"
      ],
      "metadata": {
        "id": "7Eg8GeDbe6xx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile lzw.py\n",
        "import os\n",
        "from typing import Tuple, List\n",
        "import struct\n",
        "import time\n",
        "import tracemalloc\n",
        "import json\n",
        "import argparse\n",
        "import json\n",
        "import sys\n",
        "from pickle import FALSE\n",
        "\n",
        "class CompactTrieNode:\n",
        "  def __init__(self, binary_string: str = \"\", value: str = \"\", is_leaf: bool = False) -> None:\n",
        "    self.children = [None, None]\n",
        "    self.is_leaf = is_leaf\n",
        "    self.binary_string = binary_string\n",
        "    self.value = value\n",
        "    self.unique_id = id(self)  # Temporário\n",
        "\n",
        "class CompactTrie:\n",
        "  def __init__(self) -> None:\n",
        "    self.root = None\n",
        "\n",
        "  def get_common_prefix_length(self, key1: str, key2: str) -> int:\n",
        "    i = 0\n",
        "    while i < min(len(key1), len(key2)) and key1[i] == key2[i]:\n",
        "        i += 1\n",
        "    return i\n",
        "\n",
        "  def search(self, key: str) -> CompactTrieNode:\n",
        "    current_node = self.root\n",
        "\n",
        "    if(current_node == None):\n",
        "      return None\n",
        "\n",
        "    while True:\n",
        "\n",
        "      if(current_node.binary_string == key):\n",
        "        if(current_node.is_leaf == True):\n",
        "          return current_node\n",
        "        else:\n",
        "          return None\n",
        "\n",
        "      common_prefix_length = self.get_common_prefix_length(current_node.binary_string, key)\n",
        "\n",
        "      if(common_prefix_length == len(current_node.binary_string)):\n",
        "\n",
        "        key = key[common_prefix_length:]\n",
        "\n",
        "        if current_node.children[int(key[0])] == None:\n",
        "          return None\n",
        "\n",
        "        current_node = current_node.children[int(key[0])]\n",
        "      else:\n",
        "        return None\n",
        "\n",
        "  def insert(self, key: str, value: str) -> None:\n",
        "    # Caso em que não tínhamos raiz\n",
        "    if(self.root == None):\n",
        "      self.root = CompactTrieNode(key, value, True)\n",
        "      return\n",
        "\n",
        "    else:\n",
        "      current_node = self.root\n",
        "\n",
        "      while True:\n",
        "        # Achei, posso parar\n",
        "        if(current_node.binary_string == key):\n",
        "          current_node.is_leaf = True\n",
        "          return\n",
        "\n",
        "        common_prefix_length = self.get_common_prefix_length(current_node.binary_string, key)\n",
        "\n",
        "        # Sobrou parte da key ainda, com certeza\n",
        "        if (common_prefix_length == len(current_node.binary_string)):\n",
        "          key = key[common_prefix_length:]\n",
        "\n",
        "          if current_node.children[int(key[0])] == None:\n",
        "            current_node.children[int(key[0])] = CompactTrieNode(key, value, True)\n",
        "            return\n",
        "\n",
        "          current_node = current_node.children[int(key[0])]\n",
        "\n",
        "        # Achei onde vou ter que criar novo nó para inserir\n",
        "        else:\n",
        "          key = key[common_prefix_length:]\n",
        "\n",
        "          if(current_node.is_leaf == True):\n",
        "            old_suffix_node = CompactTrieNode(current_node.binary_string[common_prefix_length:], current_node.value, True)\n",
        "          else:\n",
        "            old_suffix_node = CompactTrieNode(current_node.binary_string[common_prefix_length:], current_node.value)\n",
        "\n",
        "          old_suffix_node.children[0] = current_node.children[0]\n",
        "          old_suffix_node.children[1] = current_node.children[1]\n",
        "\n",
        "          current_node.binary_string = current_node.binary_string[:common_prefix_length]\n",
        "\n",
        "          if(len(key) > 0):\n",
        "            key_suffix_node = CompactTrieNode(key, value, True)\n",
        "            current_node.is_leaf = False\n",
        "            current_node.children[int(key_suffix_node.binary_string[0])] = key_suffix_node\n",
        "            current_node.children[int(old_suffix_node.binary_string[0])] = old_suffix_node\n",
        "          else:\n",
        "            current_node.value = value\n",
        "            current_node.is_leaf = True\n",
        "            current_node.children = [None, None]\n",
        "            current_node.children[int(old_suffix_node.binary_string[0])] = old_suffix_node\n",
        "          return\n",
        "\n",
        "  def delete_key(self, key: str) -> None:\n",
        "    if(self.search(key) == None):\n",
        "      return\n",
        "\n",
        "    current_node = self.root\n",
        "    last_node = self.root\n",
        "\n",
        "    if(current_node.binary_string == key):\n",
        "      if(current_node.children[0] == None and current_node.children[1] == None):\n",
        "        self.root = None\n",
        "        return\n",
        "      elif(current_node.children[0] != None and current_node.children[1] != None):\n",
        "        current_node.is_leaf = False\n",
        "        return\n",
        "      else:\n",
        "        valid_child = 0 if current_node.children[0] != None else 1\n",
        "        current_node.children[valid_child].binary_string = current_node.binary_string + current_node.children[valid_child].binary_string\n",
        "        self.root = current_node.children[valid_child]\n",
        "        return\n",
        "\n",
        "    while True:\n",
        "      if((current_node.binary_string == key) and (current_node.is_leaf == True)):\n",
        "        if((current_node.children[0] != None) and (current_node.children[1] != None)):\n",
        "          current_node.is_leaf = False\n",
        "        elif((current_node.children[0] == None) and (current_node.children[1] == None)):\n",
        "          last_node.children[int(current_node.binary_string[0])] = None\n",
        "\n",
        "          if(last_node.is_leaf == False):\n",
        "            last_node_other_child = 0 if current_node.binary_string[0] == '1' else 1\n",
        "\n",
        "            if(last_node.children[last_node_other_child] != None):\n",
        "              last_node.binary_string += last_node.children[last_node_other_child].binary_string\n",
        "              last_node.value = last_node.children[last_node_other_child].value\n",
        "              if(last_node.children[last_node_other_child].is_leaf):\n",
        "                last_node.is_leaf = True\n",
        "              last_node.children = last_node.children[last_node_other_child].children\n",
        "        else:\n",
        "          valid_child = 0 if current_node.children[0] != None else 1\n",
        "          current_node.children[valid_child].binary_string = current_node.binary_string + current_node.children[valid_child].binary_string\n",
        "          last_node.children[int(current_node.children[valid_child].binary_string[0])] = current_node.children[valid_child]\n",
        "        return\n",
        "\n",
        "      common_prefix_length = self.get_common_prefix_length(current_node.binary_string, key)\n",
        "      key = key[common_prefix_length:]\n",
        "      last_node = current_node\n",
        "      current_node = current_node.children[int(key[0])]\n",
        "\n",
        "  # Temporário\n",
        "  def visualize(self, filename=\"compact_trie\"):\n",
        "      dot = Digraph(comment=\"Compact Trie\")\n",
        "\n",
        "      def add_nodes_edges(node, parent_label=None):\n",
        "          if node is None:\n",
        "              return\n",
        "\n",
        "          new_node_binary_string = node.binary_string\n",
        "          new_node_binary_string = new_node_binary_string.replace(\"0\", \"a\")\n",
        "          new_node_binary_string = new_node_binary_string.replace(\"1\", \"b\")\n",
        "\n",
        "          node_label = f\"{new_node_binary_string}_{node.unique_id}\"\n",
        "\n",
        "          display_label = f\"{new_node_binary_string}\"\n",
        "          if(node.is_leaf):\n",
        "            display_label  += f\"\\\\nValue: {node.value}\"\n",
        "\n",
        "          dot.node(node_label, display_label, shape='circle', color='black', fontcolor='red' if node.is_leaf else 'blue')\n",
        "\n",
        "          if parent_label:\n",
        "              dot.edge(parent_label, node_label)\n",
        "          if node.children[0] is not None:\n",
        "              add_nodes_edges(node.children[0], node_label)\n",
        "          if node.children[1] is not None:\n",
        "              add_nodes_edges(node.children[1], node_label)\n",
        "\n",
        "      if self.root is not None:\n",
        "          add_nodes_edges(self.root)\n",
        "\n",
        "      dot.render(filename, format=\"png\", cleanup=True)\n",
        "      print(f\"Compact trie saved as {filename}.png\")\n",
        "\n",
        "# Para debug\n",
        "def binary_to_chars(binary_string):\n",
        "    # Ensure the binary string length is a multiple of 8 (since each character is 8 bits)\n",
        "    if len(binary_string) % 8 != 0:\n",
        "        raise ValueError(\"Binary string length must be a multiple of 8.\")\n",
        "\n",
        "    # Split the binary string into chunks of 8 bits\n",
        "    chars = [binary_string[i:i+8] for i in range(0, len(binary_string), 8)]\n",
        "\n",
        "    # Convert each 8-bit binary chunk to its corresponding character\n",
        "    result = ''.join(chr(int(char, 2)) for char in chars)\n",
        "\n",
        "    return result\n",
        "\n",
        "# Função para converter um conjunto de bytes em um '.txt'\n",
        "def write_txt_file(decoded_bytes, output_file):\n",
        "    decoded_string = ''.join([chr(byte) for byte in decoded_bytes])\n",
        "    with open(output_file, 'w', encoding='utf-8') as file:\n",
        "        file.write(decoded_string)\n",
        "\n",
        "# Função para converter um conjunto de bytes em um '.bmp' ou '.tiff'\n",
        "def write_image_file(decoded_bytes, output_file):\n",
        "    with open(output_file, 'wb') as file:\n",
        "        file.write(bytes(decoded_bytes))\n",
        "\n",
        "# Função para remover zeros à esquerda em uma string binárias\n",
        "def remove_leading_zeros(binary_str):\n",
        "  return binary_str.lstrip('0') or '0'\n",
        "\n",
        "# Classe que implementa o algoritmo LZW, empregando árvores Tries Binárias como dicionários\n",
        "class TrieLZW:\n",
        "\n",
        "  # Atributos para armazenar as estatísticas\n",
        "  def __init__(self):\n",
        "      self.stats = {\n",
        "          'compression_ratio_over_time': [],\n",
        "          'dictionary_size_over_time': [],\n",
        "          'execution_time': 0,\n",
        "          'memory_usage': 0\n",
        "      }\n",
        "\n",
        "  # Realiza a compressão LZW do arquivo passado como parâmetro\n",
        "  def compress(self, file_path, compression_type, codes_max_size) -> Tuple[str, str, List]:\n",
        "\n",
        "    # Iniciar rastreamento de tempo e memória\n",
        "    start_time = time.time()\n",
        "    tracemalloc.start()\n",
        "\n",
        "    # Utilizando a Trie Compacta Binária como dicionário no algoritmo\n",
        "    dictionary = CompactTrie()\n",
        "\n",
        "    # Inicializando todas as chaves de 00000000 até 11111111 no dicionário, com respectivos valores sendo a própria chave\n",
        "    for num in range(256):\n",
        "      byte_num = format(num, '08b')\n",
        "      dictionary.insert(byte_num, byte_num)\n",
        "\n",
        "    dict_size = 256 # O dicionário começa com todos os símbolos ASCII\n",
        "    reset = False\n",
        "\n",
        "    if(compression_type == \"s\"):\n",
        "      num_bits_values = 12 # No caso estático, os códigos terão tamanho 12 bits\n",
        "      dict_size_limit = 2 ** num_bits_values # Os 12 bits serão suficientes para representar códigos de 000000000000 (0) até 111111111111 (4095)\n",
        "    else:\n",
        "      num_bits_values = 9 # No caso estático dinâmico, os códigos terão tamanho 9 bits inicialmente\n",
        "      next_dict_size_limit = 512 # Os 9 bits serão suficientes para representar códigos de 000000000 (0) até 111111111 (511)\n",
        "\n",
        "    # Inicializando variáveis utilizadas pela compressão LZW\n",
        "    string = \"\"\n",
        "    compressed_data = []\n",
        "    resets = []\n",
        "\n",
        "    count = 0\n",
        "    # Aplicando a compressão LZW, considerando cada byte do arquivo original como um símbolo de entrada\n",
        "    try:\n",
        "      with open(file_path, \"rb\") as file:\n",
        "        while (byte := file.read(1)):\n",
        "          #print(count)\n",
        "          count += 1\n",
        "          symbol = bin(int.from_bytes(byte, \"big\"))[2:].zfill(8)\n",
        "\n",
        "          string_plus_symbol = string + symbol\n",
        "\n",
        "          if dictionary.search(string_plus_symbol) != None:\n",
        "              string = string_plus_symbol\n",
        "          else:\n",
        "            # Reiniciando o dicionário no algoritmo estático\n",
        "            if(compression_type == \"s\"):\n",
        "              if(dict_size == dict_size_limit - 1):\n",
        "                reset = True\n",
        "\n",
        "            # Expandindo os códigos no algoritmo dinâmico (até o limite) ou reiniciando o dicionário\n",
        "            else:\n",
        "              if(dict_size == (2 ** codes_max_size) - 1):\n",
        "                reset = True\n",
        "              else:\n",
        "                if(dict_size == next_dict_size_limit):\n",
        "                    num_bits_values += 1\n",
        "                    next_dict_size_limit *= 2\n",
        "\n",
        "            current_string_formatted_binary_value = dictionary.search(string).value.zfill(num_bits_values)\n",
        "            compressed_data.append(current_string_formatted_binary_value)\n",
        "            new_key_value = bin(dict_size)[2:].zfill(num_bits_values)\n",
        "            dictionary.insert(string_plus_symbol, new_key_value)\n",
        "            dict_size += 1\n",
        "            string = symbol\n",
        "\n",
        "            if(reset):\n",
        "              reset = False\n",
        "              dict_size = 256\n",
        "\n",
        "              if(compression_type != 's'):\n",
        "                num_bits_values = 9\n",
        "                next_dict_size_limit = 512\n",
        "\n",
        "              dictionary = CompactTrie()\n",
        "\n",
        "              for num in range(256):\n",
        "                byte_num = format(num, '08b')\n",
        "                dictionary.insert(byte_num, byte_num)\n",
        "\n",
        "        if(dictionary.search(string) != None):\n",
        "          current_string_formatted_binary_value = dictionary.search(string).value.zfill(num_bits_values)\n",
        "          compressed_data.append(current_string_formatted_binary_value)\n",
        "\n",
        "    # Tratando erros que podem ocorrer na abertura de um arquivo\n",
        "    except FileNotFoundError as e:\n",
        "      print(f\"Arquivo não encontrado -> {e}\")\n",
        "    except IOError as e:\n",
        "      print(f\"Erro de I/O -> {e}\")\n",
        "    except Exception as e:\n",
        "      print(f\"Um erro ocorreu: {e}\")\n",
        "\n",
        "    # Salvando o nome do arquivo original e sua extensão para que o arquivo de compressão possa ser salvo\n",
        "    file_name, file_extension = os.path.splitext(file_path)\n",
        "\n",
        "    with open(f\"compressed_{file_name}.bin\", \"wb\") as file:\n",
        "      # Convertendo os códigos para uma única string\n",
        "      final_concat_string_data = ''.join(compressed_data)\n",
        "\n",
        "      # Verificando se algum padding deverá ser adicionado para que tenhamos um número inteiro de bytes no arquivo comprimido\n",
        "      padding_size = (8 - len(final_concat_string_data) % 8) % 8\n",
        "      final_concat_string_data = '0' * padding_size + final_concat_string_data\n",
        "\n",
        "      # Adicionando uma flag, no arquivo comprimido, para representar o tamanho do padding que foi adicionado\n",
        "      file.write(bytes([padding_size]))\n",
        "\n",
        "      # Convertendo a string de dados comprimidos para bytes e escrevendo no arquivo comprimido final\n",
        "      bits = []\n",
        "      for char in final_concat_string_data:\n",
        "        bits.append(1 if char == '1' else 0)\n",
        "      byte = 0\n",
        "      bit_count = 0\n",
        "      for bit in bits:\n",
        "        byte = (byte << 1) | bit\n",
        "        bit_count += 1\n",
        "        if bit_count == 8:\n",
        "          file.write(bytes([byte]))\n",
        "          byte = 0\n",
        "          bit_count = 0\n",
        "\n",
        "      # Finalizar rastreamento de tempo e memória\n",
        "      end_time = time.time()\n",
        "      self.stats['execution_time'] = end_time - start_time\n",
        "\n",
        "      current_memory, peak_memory = tracemalloc.get_traced_memory()\n",
        "      self.stats['memory_usage'] = peak_memory / 1024  # Converter para KB\n",
        "      tracemalloc.stop()\n",
        "\n",
        "      return file_name, file_extension\n",
        "\n",
        "  # Realiza a descompressão LZW do arquivo passado como parâmetro\n",
        "  def decompress(self, file_path, compression_type, original_file_name, original_file_extension, codes_max_size) -> None:\n",
        "\n",
        "    # Iniciar rastreamento de tempo e memória\n",
        "    start_time = time.time()\n",
        "    tracemalloc.start()\n",
        "\n",
        "    # Utilizando a Trie Compacta Binária como dicionário no algoritmo\n",
        "    dictionary = CompactTrie()\n",
        "\n",
        "    # Inicializando todas as chaves de 0, 1, 10, 11, 100, ... até 11111111 no dicionário (sem padding), com respectivos valores sendo a própria chave (com padding)\n",
        "    for num in range(256):\n",
        "      numeric_key = bin(num)[2:]\n",
        "      numeric_key_8bit_value = format(num, '08b')\n",
        "      dictionary.insert(numeric_key, numeric_key_8bit_value)\n",
        "\n",
        "    dict_size = 256 # O dicionário começa com todos os símbolos ASCII\n",
        "    reset = False\n",
        "\n",
        "    if(compression_type == \"s\"):\n",
        "      decompression_size = 12 # Sempre puxaremos 12 bits do arquivo comprimido por vez no caso estático\n",
        "      dict_size_limit = 2 ** decompression_size # Os 12 bits serão suficientes para representar códigos de 000000000000 (0) até 111111111111 (4095)\n",
        "    else:\n",
        "      decompression_size = 9 # Começaremos puxando 9 bits do arquivo comprimido por vez\n",
        "      next_size_limit = 512 # Os 9 bits serão suficientes para representar códigos de 000000000 (0) até 111111111 (511)\n",
        "\n",
        "    # Abrindo o arquivo já comprimido e convertendo seus bytes em uma única string\n",
        "    compressed_data = \"\"\n",
        "    try:\n",
        "      with open(file_path, \"rb\") as file:\n",
        "        file_data = file.read()\n",
        "        compressed_data = ''.join(format(byte, '08b') for byte in file_data)\n",
        "    # Tratando erros que podem ocorrer na abertura de um arquivo\n",
        "    except FileNotFoundError:\n",
        "      print(f\"File not found: {file_path}\")\n",
        "    except IOError as e:\n",
        "      print(f\"Error reading the file: {e}\")\n",
        "    except Exception as e:\n",
        "      print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "    # Retirando os bits de padding do arquivo comprimido\n",
        "    padding_size = int(compressed_data[:8], 2)\n",
        "    compressed_data = compressed_data[(8 + padding_size):]\n",
        "\n",
        "    # Inicializando as variáveis que serão empregadas posteriormente\n",
        "    string = compressed_data[:decompression_size]\n",
        "    string = string[-8:]\n",
        "    compressed_data = compressed_data[decompression_size:]\n",
        "    decompressed_data = [string]\n",
        "\n",
        "    count = 0\n",
        "    # Aplicando a descompressão LZW\n",
        "    while(len(compressed_data) > 0):\n",
        "        #print(count)\n",
        "        count += 1\n",
        "        if(compression_type == \"s\"):\n",
        "          # Caso em que os códigos têm tamanho estático\n",
        "          if(dict_size == dict_size_limit - 1):\n",
        "            reset = True\n",
        "        else:\n",
        "          # Caso em que os códigos têm tamanho dinâmico\n",
        "          if(dict_size == 2 ** codes_max_size - 1):\n",
        "            decompression_size = 9\n",
        "            reset = True\n",
        "          else:\n",
        "            if(dict_size == next_size_limit - 1):\n",
        "              decompression_size += 1\n",
        "              next_size_limit *= 2\n",
        "\n",
        "        k = compressed_data[:decompression_size]\n",
        "        compressed_data = compressed_data[decompression_size:]\n",
        "        k = remove_leading_zeros(k)\n",
        "\n",
        "        if dictionary.search(k) != None:\n",
        "          entry = dictionary.search(k).value\n",
        "        else:\n",
        "          new_value_entry_concat = string[:8]\n",
        "          entry = string + new_value_entry_concat\n",
        "\n",
        "        decompressed_data.append(entry)\n",
        "        new_key = bin(dict_size)[2:] if dict_size != 0 else '0'\n",
        "        dictionary.insert(new_key, string + entry[:8])\n",
        "        dict_size += 1\n",
        "        string = entry\n",
        "\n",
        "        if(reset):\n",
        "          reset = False\n",
        "          dict_size = 256\n",
        "\n",
        "          if(compression_type != \"s\"):\n",
        "            decompression_size = 9\n",
        "            next_size_limit = 512\n",
        "\n",
        "          dictionary = CompactTrie()\n",
        "          for num in range(256):\n",
        "            numeric_key = bin(num)[2:]\n",
        "            numeric_key_8bit_value = format(num, '08b')\n",
        "            dictionary.insert(numeric_key, numeric_key_8bit_value)\n",
        "\n",
        "    decompressed_concat_string = ''.join(decompressed_data)\n",
        "\n",
        "    # Checando algum possível erro que possa ter ocorrido na descompressão (o tamanho do arquivo final deve ser um múltiplo de 8)\n",
        "    if len(decompressed_concat_string) % 8 != 0:\n",
        "      raise ValueError(\"O arquivo descomprimido não tem um número inteiro de bytes!\")\n",
        "\n",
        "    # Transformando a string binárias em bytes individuais\n",
        "    byte_chunks = [decompressed_concat_string[i:i+8] for i in range(0, len(decompressed_concat_string), 8)]\n",
        "    bytes_ = [int(chunk, 2) for chunk in byte_chunks]\n",
        "\n",
        "    # Convert integers to their ASCII characters\n",
        "    ascii_characters = [chr(i) for i in bytes_]\n",
        "    # Join characters into a string (optional)\n",
        "    ascii_string = ''.join(ascii_characters)\n",
        "\n",
        "    # Gerando o arquivo inicial\n",
        "    switch = {\n",
        "        '.txt': write_txt_file,\n",
        "        '.bmp': write_image_file,\n",
        "        '.tiff': write_image_file\n",
        "        }\n",
        "    handler = switch.get(original_file_extension)\n",
        "    if handler:\n",
        "        handler(bytes_, f\"decompressed_{original_file_name}{original_file_extension}\")\n",
        "    else:\n",
        "        print(\"Tipo de arquivo não suportado!\")\n",
        "\n",
        "    # Finalizar rastreamento de tempo e memória\n",
        "    end_time = time.time()\n",
        "    self.stats['execution_time'] = end_time - start_time\n",
        "\n",
        "    current_memory, peak_memory = tracemalloc.get_traced_memory()\n",
        "    self.stats['memory_usage'] = peak_memory / 1024  # Converter para KB\n",
        "    tracemalloc.stop()\n",
        "\n",
        "# Funções de Nível Superior para Compressão e Descompressão\n",
        "def compress(input_file, output_file, compression_type, max_bits, stats_file=None):\n",
        "    print(f\"Compressão de '{input_file}'...\")\n",
        "    trie_lzw = TrieLZW()\n",
        "\n",
        "    # Executa a compressão com os parâmetros especificados\n",
        "    file_name, file_extension = trie_lzw.compress(input_file, compression_type, max_bits)\n",
        "    compressed_file_name = f\"compressed_{file_name}.bin\"\n",
        "    print(f\"Arquivo comprimido criado: {compressed_file_name}\")\n",
        "\n",
        "    # Define output_file como o nome do arquivo comprimido gerado\n",
        "    output_file = compressed_file_name\n",
        "    print(f\"Compressão concluída: {output_file}\")\n",
        "\n",
        "    # Salva as estatísticas, se especificado\n",
        "    if stats_file:\n",
        "        with open(stats_file, 'w') as f:\n",
        "            json.dump(trie_lzw.stats, f, indent=4)\n",
        "        print(f\"Estatísticas de compressão salvas em '{stats_file}'\")\n",
        "\n",
        "def decompress(input_file, output_file, compression_type, max_bits, stats_file=None):\n",
        "    print(f\"Descompressão de '{input_file}'...\")\n",
        "    trie_lzw = TrieLZW()\n",
        "\n",
        "    # Extrai o nome base e a extensão do arquivo de saída desejado\n",
        "    output_base_name, output_extension = os.path.splitext(os.path.basename(output_file))\n",
        "    original_file_name = output_base_name.replace('decompressed_', '')\n",
        "\n",
        "    # Executa a descompressão\n",
        "    trie_lzw.decompress(input_file, compression_type, original_file_name, output_extension, max_bits)\n",
        "\n",
        "    # Define decompressed_file_name conforme gerado pela descompressão\n",
        "    decompressed_file_name = f\"decompressed_{original_file_name}{output_extension}\"\n",
        "    print(f\"Arquivo descomprimido criado: {decompressed_file_name}\")\n",
        "\n",
        "    # Define output_file como o nome do arquivo descomprimido gerado\n",
        "    output_file = decompressed_file_name\n",
        "    print(f\"Descompressão concluída: {output_file}\")\n",
        "\n",
        "    # Salva as estatísticas, se especificado\n",
        "    if stats_file:\n",
        "        with open(stats_file, 'w') as f:\n",
        "            json.dump(trie_lzw.stats, f, indent=4)\n",
        "        print(f\"Estatísticas de descompressão salvas em '{stats_file}'\")\n",
        "\n",
        "# Função `process` para Compressão e Descompressão Sequencial\n",
        "def process(input_file, output_file, compression_type, max_bits=12, stats_file=None):\n",
        "    print(\"Compressão...\")\n",
        "    trie_lzw = TrieLZW()\n",
        "\n",
        "    # Executa a compressão\n",
        "    file_name, file_extension = trie_lzw.compress(input_file, compression_type, max_bits)\n",
        "    compressed_file_name = f\"compressed_{file_name}.bin\"\n",
        "    print(f\"Arquivo comprimido criado: {compressed_file_name}\")\n",
        "\n",
        "    # Define output_file como o nome do arquivo comprimido gerado\n",
        "    temp_compressed_file = compressed_file_name\n",
        "    # print(f\"Usando o arquivo comprimido '{temp_compressed_file}' para descompressão\")\n",
        "\n",
        "    # Salva as estatísticas da compressão, se especificado\n",
        "    if stats_file:\n",
        "        with open(stats_file, 'w') as f:\n",
        "            json.dump(trie_lzw.stats, f, indent=4)\n",
        "        print(f\"Estatísticas de compressão salvas em '{stats_file}'\")\n",
        "\n",
        "    print(\"Descompressão...\")\n",
        "    # Extrai o nome base e a extensão do arquivo de saída desejado\n",
        "    output_base_name, output_extension = os.path.splitext(os.path.basename(output_file))\n",
        "    original_file_name = output_base_name.replace('decompressed_', '')\n",
        "\n",
        "    # Executa a descompressão\n",
        "    trie_lzw.decompress(temp_compressed_file, compression_type, original_file_name, output_extension, max_bits)\n",
        "\n",
        "    # Define decompressed_file_name conforme gerado pela descompressão\n",
        "    decompressed_file_name = f\"decompressed_{original_file_name}{output_extension}\"\n",
        "    # print(f\"Arquivo descomprimido criado: {decompressed_file_name}\")\n",
        "\n",
        "    # Define output_file como o nome do arquivo descomprimido gerado\n",
        "    output_file = decompressed_file_name\n",
        "    print(f\"Descompressão concluída: {output_file}\")\n",
        "\n",
        "    # Salva as estatísticas da descompressão, se especificado\n",
        "    if stats_file:\n",
        "        with open(stats_file, 'w') as f:\n",
        "            json.dump(trie_lzw.stats, f, indent=4)\n",
        "        print(f\"Estatísticas de descompressão salvas em '{stats_file}'\")\n",
        "\n",
        "    print(f\"Processo concluído.\")\n",
        "\n",
        "# Função `main` com Suporte para Processamento Sequencial\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"Compressão e Descompressão LZW\")\n",
        "    subparsers = parser.add_subparsers(dest='command', help='Comandos disponíveis: compress, decompress, process')\n",
        "\n",
        "    # Parser para o comando 'compress'\n",
        "    compress_parser = subparsers.add_parser('compress', help='Comprimir um arquivo')\n",
        "    compress_parser.add_argument('input_file', type=str, help='Caminho para o arquivo de entrada a ser comprimido')\n",
        "    compress_parser.add_argument('output_file', type=str, help='Caminho para o arquivo comprimido de saída')\n",
        "    compress_parser.add_argument('--type', type=str, choices=['s', 'd'], required=True, help='Tipo de compressão: \"s\" para tamanho fixo, \"d\" para tamanho variável')\n",
        "    compress_parser.add_argument('--max-bits', type=int, default=12, help='Número máximo de bits (padrão: 12)')\n",
        "    compress_parser.add_argument('--stats-file', type=str, help='Caminho para salvar as estatísticas de compressão', default=None)\n",
        "\n",
        "    # Parser para o comando 'decompress'\n",
        "    decompress_parser = subparsers.add_parser('decompress', help='Descomprimir um arquivo')\n",
        "    decompress_parser.add_argument('input_file', type=str, help='Caminho para o arquivo comprimido de entrada')\n",
        "    decompress_parser.add_argument('output_file', type=str, help='Caminho para o arquivo descomprimido de saída')\n",
        "    decompress_parser.add_argument('--type', type=str, choices=['s', 'd'], required=True, help='Tipo de compressão: \"s\" para tamanho fixo, \"d\" para tamanho variável')\n",
        "    decompress_parser.add_argument('--max-bits', type=int, default=12, help='Número máximo de bits (padrão: 12)')\n",
        "    decompress_parser.add_argument('--stats-file', type=str, help='Caminho para salvar as estatísticas de descompressão', default=None)\n",
        "\n",
        "    # Parser para o comando 'process'\n",
        "    process_parser = subparsers.add_parser('process', help='Comprimir e descomprimir um arquivo sequencialmente')\n",
        "    process_parser.add_argument('input_file', type=str, help='Caminho para o arquivo de entrada a ser comprimido e descomprimido')\n",
        "    process_parser.add_argument('output_file', type=str, help='Caminho para o arquivo descomprimido final de saída')\n",
        "    process_parser.add_argument('--type', type=str, choices=['s', 'd'], required=True, help='Tipo de compressão: \"s\" para tamanho fixo, \"d\" para tamanho variável')\n",
        "    process_parser.add_argument('--max-bits', type=int, default=12, help='Número máximo de bits (padrão: 12)')\n",
        "    process_parser.add_argument('--stats-file', type=str, help='Caminho para salvar as estatísticas de compressão e descompressão', default=None)\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if args.command == 'compress':\n",
        "        compress(args.input_file, args.output_file, args.type, args.max_bits, args.stats_file)\n",
        "    elif args.command == 'decompress':\n",
        "        decompress(args.input_file, args.output_file, args.type, args.max_bits, args.stats_file)\n",
        "    elif args.command == 'process':\n",
        "        process(args.input_file, args.output_file, args.type, args.max_bits, args.stats_file)\n",
        "    else:\n",
        "        parser.print_help()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0kXszE43ECO",
        "outputId": "f90dd6fd-7efe-4372-f586-65fe8819b7ef"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting lzw.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teste texto gerado"
      ],
      "metadata": {
        "id": "PIJ1NB8Xb9vs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gerando artificialmente um texto para teste:"
      ],
      "metadata": {
        "id": "SKUI832ScGIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Generate the random text\n",
        "input_data = ''.join(random.choices(\"abcd\", k=100000))\n",
        "\n",
        "#input_data = 'geekific-geekific'\n",
        "\n",
        "# Specify the file path\n",
        "file_path = \"test_text.txt\"\n",
        "\n",
        "# Save the text to a .txt file\n",
        "with open(file_path, \"w\") as file:\n",
        "    file.write(input_data)"
      ],
      "metadata": {
        "id": "16IAqmH62hmi"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aplicando a compressão e a descompressão em sequência:"
      ],
      "metadata": {
        "id": "0SW2Dn8rcI4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python lzw.py process test_text.txt saida_final.txt --type d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvkbAbZz6Xvw",
        "outputId": "44afa94b-d038-414b-eab7-f96d6e907fca"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compressão...\n",
            "Arquivo comprimido criado: compressed_test_text.bin\n",
            "Descompressão...\n",
            "Descompressão concluída: decompressed_saida_final.txt\n",
            "Processo concluído.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparando o tamanho dos arquivos iniciais e finais:"
      ],
      "metadata": {
        "id": "-mGEIDUicLru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcula e imprime os tamanhos dos arquivos original e comprimido\n",
        "original_file_size = os.path.getsize(\"test_text.txt\")\n",
        "compressed_file_size = os.path.getsize(\"compressed_test_text.bin\")\n",
        "print(f\"Tamanho do arquivo original: {original_file_size} bytes.\")\n",
        "print(f\"Tamanho do arquivo comprimido: {compressed_file_size} bytes.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5J-L0k_6s3A",
        "outputId": "b4fb85e0-0426-4c87-e807-f6f3073de82d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamanho do arquivo original: 100000 bytes.\n",
            "Tamanho do arquivo comprimido: 29883 bytes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verificando se os arquivos são equivalentes:"
      ],
      "metadata": {
        "id": "qjO4c79_cNmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verifica se os arquivos original e descomprimido são iguais\n",
        "def are_files_identical(file1, file2):\n",
        "    with open(file1, 'r') as f1, open(file2, 'r') as f2:\n",
        "        return f1.read() == f2.read()\n",
        "\n",
        "txt_file1 = \"test_text.txt\"\n",
        "txt_file2 = \"decompressed_saida_final.txt\"\n",
        "print(f\"Os arquivos .txt são iguais? {are_files_identical(txt_file1, txt_file2)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPDUsyV6bjqx",
        "outputId": "8b8a267f-0f94-4371-a5ed-496e5ea270a8"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Os arquivos .txt são iguais? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teste imagem gerada"
      ],
      "metadata": {
        "id": "jAi4cZAJcObl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "# Create a new monochrome (mode '1') image with 10x10 pixels\n",
        "width, height = 1500, 1500\n",
        "image = Image.new(\"1\", (width, height))  # Mode \"1\" means 1-bit pixels (monochrome)\n",
        "\n",
        "# Set some pixels to black (1 = white, 0 = black)\n",
        "pixels = image.load()\n",
        "for x in range(width):\n",
        "    for y in range(height):\n",
        "        if (x + y) % 2 == 0:  # Example pattern: checkerboard\n",
        "            pixels[x, y] = 0  # Black pixel\n",
        "        else:\n",
        "            pixels[x, y] = 1  # White pixel\n",
        "\n",
        "# Save the image as a BMP file\n",
        "image.save(\"monochrome_test_image.bmp\")"
      ],
      "metadata": {
        "id": "MGkNbjFRb7nS"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aplicando a compressão e a descompressão em sequência:"
      ],
      "metadata": {
        "id": "Byxq6eg9dYZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python lzw.py process imagemhomog_30kb.bmp imagemhomog_des.bmp --type s"
      ],
      "metadata": {
        "id": "V13GU0grfnAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparando o tamanho dos arquivos iniciais e finais:"
      ],
      "metadata": {
        "id": "PMeoa3WrdXJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original_file_size = os.path.getsize(\"imagemhomog_30kb.bmp\")\n",
        "compressed_file_size = os.path.getsize(\"compressed_monochrome_test_image.bin\")\n",
        "\n",
        "print(f\"Tamanho do arquivo original: {original_file_size} bytes.\")\n",
        "print(f\"Tamanho do arquivo comprimido: {compressed_file_size} bytes.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Foub5V3acYxS",
        "outputId": "f70a20fc-63ab-4fdb-93b2-abb9b2b543ff"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamanho do arquivo original: 282062 bytes.\n",
            "Tamanho do arquivo comprimido: 3417 bytes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verificando se os arquivos são equivalentes:"
      ],
      "metadata": {
        "id": "l9rvYMt-ddR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python lzw.py process imagemhomog_30kb.bmp imagemhomog_des.bmp --type s"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXeHscqVfvzD",
        "outputId": "096ea9a8-1ce2-4936-d571-c1c54739d8f5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compressão...\n",
            "Arquivo comprimido criado: compressed_imagemhomog_30kb.bin\n",
            "Descompressão...\n",
            "Descompressão concluída: decompressed_imagemhomog_des.bmp\n",
            "Processo concluído.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Arquivos de teste"
      ],
      "metadata": {
        "id": "j-nsM7_Udg4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar o gdown\n",
        "!pip install gdown\n",
        "\n",
        "# Baixar o arquivo usando o ID do arquivo\n",
        "!gdown --id 1MK1qVVt3kmuohGnZVx2riHmVaKuGvs_z\n",
        "\n",
        "# # Descompactar o arquivo\n",
        "print(\"Descompactando arquivo. Aguarde...\")\n",
        "!unzip -oq 'test_cases.zip' -d './'\n",
        "print(\"Descompactação concluída.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_EdnsNyUPo1",
        "outputId": "291258a5-1840-4c61-99e8-e7e84c1b1b9a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.6)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.8.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1MK1qVVt3kmuohGnZVx2riHmVaKuGvs_z\n",
            "To: /content/test_cases.zip\n",
            "100% 935k/935k [00:00<00:00, 98.0MB/s]\n",
            "Descompactando arquivo. Aguarde...\n",
            "Descompactação concluída.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import filecmp\n",
        "\n",
        "# Configurações para melhor visualização dos gráficos\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (18, 12)  # Aumenta o tamanho padrão das figuras\n",
        "\n",
        "# Diretório onde os arquivos de teste estão localizados\n",
        "test_dir = 'test_cases'\n",
        "\n",
        "# Diretório onde os arquivos comprimidos serão salvos\n",
        "compressed_dir = 'compressed_test_cases'\n",
        "\n",
        "# Verifica se o diretório 'test_cases' existe\n",
        "if not os.path.exists(test_dir):\n",
        "    print(f\"O diretório '{test_dir}' não existe. Por favor, certifique-se de que os arquivos de teste estão na pasta '{test_dir}'.\")\n",
        "    exit(1)\n",
        "\n",
        "# Cria o diretório 'compressed_test_cases' se não existir\n",
        "if not os.path.exists(compressed_dir):\n",
        "    os.makedirs(compressed_dir)\n",
        "    print(f\"Diretório '{compressed_dir}' criado para armazenar os arquivos comprimidos.\")\n",
        "\n",
        "# Lista para armazenar os arquivos que precisam ser processados\n",
        "files_to_process = []\n",
        "\n",
        "# Extensões de arquivos a serem considerados\n",
        "valid_extensions = ['.txt', '.bmp', '.bin', '.pgm', '.wav']\n",
        "\n",
        "# Percorrer todos os arquivos no diretório 'test_cases'\n",
        "for filename in os.listdir(test_dir):\n",
        "    file_path = os.path.join(test_dir, filename)\n",
        "    # Ignorar diretórios\n",
        "    if os.path.isdir(file_path):\n",
        "        continue\n",
        "    # Ignorar arquivos que já foram descomprimidos\n",
        "    if '_decompressed' in filename:\n",
        "        continue\n",
        "    # Ignorar arquivos de estatísticas ou outros arquivos não relevantes\n",
        "    if filename.endswith('_process_stats.json'):\n",
        "        continue\n",
        "    # Verificar se a extensão é válida\n",
        "    _, ext = os.path.splitext(filename)\n",
        "    if ext.lower() in valid_extensions:\n",
        "        files_to_process.append(filename)\n",
        "    else:\n",
        "        print(f\"Arquivo com extensão não suportada encontrado e será ignorado: {filename}\")\n",
        "\n",
        "# Função para determinar o tipo de arquivo com base na extensão\n",
        "def get_file_type(extension):\n",
        "    if extension == '.txt':\n",
        "        return 'Texto'\n",
        "    elif extension in ['.bmp', '.pgm']:\n",
        "        return 'Imagem'\n",
        "    elif extension == '.wav':\n",
        "        return 'Áudio'\n",
        "    elif extension == '.bin':\n",
        "        return 'Binário'\n",
        "    else:\n",
        "        return 'Desconhecido'\n",
        "\n",
        "# Listas para os dados de Compressão e Descompressão\n",
        "compression_data = []\n",
        "decompression_data = []\n",
        "\n",
        "# Dicionário para armazenar dados de estatísticas detalhadas para cada arquivo\n",
        "detailed_stats = {}\n",
        "\n",
        "# Automatizar o Processamento Sequencial (Compressão e Descompressão)\n",
        "for filename in files_to_process:\n",
        "    input_file = os.path.join(test_dir, filename)\n",
        "    decompressed_filename = f'{os.path.splitext(filename)[0]}_decompressed{os.path.splitext(filename)[1]}'\n",
        "    output_file = os.path.join(test_dir, decompressed_filename)\n",
        "    stats_file = os.path.join(test_dir, f'{filename}_process_stats.json')\n",
        "\n",
        "    # Verifica se o arquivo já foi descomprimido\n",
        "    if os.path.exists(output_file):\n",
        "        print(f\"Arquivo descomprimido já existe: {output_file}. Pulando o processamento deste arquivo.\")\n",
        "        continue\n",
        "\n",
        "    # Comando para executar o processo de compressão e descompressão\n",
        "    command = [\n",
        "        'python3', 'lzw.py', 'process', input_file, output_file,\n",
        "        '--type', 's',\n",
        "        '--max-bits', '12',\n",
        "        '--stats-file', stats_file\n",
        "    ]\n",
        "\n",
        "    print(f'Processando {input_file}...')\n",
        "    result = subprocess.run(command, capture_output=True, text=True)\n",
        "\n",
        "    if result.returncode != 0:\n",
        "        print(f\"Erro ao processar {input_file}:\")\n",
        "        print(result.stderr)\n",
        "    else:\n",
        "        print(result.stdout)\n",
        "\n",
        "# Verificar os Arquivos Descomprimidos\n",
        "for filename in files_to_process:\n",
        "    original_file = os.path.join(test_dir, filename)\n",
        "    original_extension = os.path.splitext(filename)[1]\n",
        "    decompressed_filename = f'{os.path.splitext(filename)[0]}_decompressed{original_extension}'\n",
        "    decompressed_file = os.path.join(test_dir, decompressed_filename)\n",
        "\n",
        "    # Verifica se o arquivo descomprimido existe\n",
        "    if not os.path.exists(decompressed_file):\n",
        "        print(f\"Arquivo descomprimido não encontrado: {decompressed_file}. Pulando verificação deste arquivo.\")\n",
        "        continue\n",
        "\n",
        "    # Verifica se o arquivo já foi descomprimido corretamente\n",
        "    if original_extension.lower() in ['.bmp', '.bin', '.pgm', '.wav']:\n",
        "        compressed_file_path = os.path.join(compressed_dir, f\"{filename}.bin\")  # Ajuste aqui\n",
        "        files_are_equal = filecmp.cmp(original_file, decompressed_file, shallow=False)\n",
        "    else:\n",
        "        # Para arquivos de texto, podemos comparar como strings\n",
        "        with open(original_file, 'r', encoding='utf-8') as f1, open(decompressed_file, 'r', encoding='utf-8') as f2:\n",
        "            files_are_equal = f1.read() == f2.read()\n",
        "\n",
        "    if files_are_equal:\n",
        "        print(f'{filename} descomprimido com sucesso.')\n",
        "    else:\n",
        "        print(f'Erro: {filename} foi descomprimido incorretamente.')\n",
        "\n",
        "# Coletar Estatísticas de Compressão e Descompressão\n",
        "for filename in files_to_process:\n",
        "    # Caminho para o arquivo de estatísticas do processamento\n",
        "    process_stats_file = os.path.join(test_dir, f'{filename}_process_stats.json')\n",
        "\n",
        "    # Determinar o tipo de arquivo com base na extensão\n",
        "    _, ext = os.path.splitext(filename)\n",
        "    ftype = get_file_type(ext.lower())\n",
        "\n",
        "    # Verifica se o arquivo de estatísticas existe\n",
        "    if os.path.exists(process_stats_file):\n",
        "        with open(process_stats_file, 'r') as f:\n",
        "            process_stats = json.load(f)\n",
        "\n",
        "        # Extraindo estatísticas de compressão\n",
        "        compress_stats = process_stats.get('compression_stats', {})\n",
        "        # Extraindo estatísticas de descompressão\n",
        "        decompress_stats = process_stats.get('decompression_stats', {})\n",
        "\n",
        "        # Calcular a taxa de compressão com base nos tamanhos dos arquivos\n",
        "        original_file_path = os.path.join(test_dir, filename)\n",
        "        compressed_file_path = os.path.join(compressed_dir, f\"{filename}.bin\")  # Ajuste aqui\n",
        "        if os.path.exists(original_file_path) and os.path.exists(compressed_file_path):\n",
        "            original_size = os.path.getsize(original_file_path)\n",
        "            compressed_size = os.path.getsize(compressed_file_path)\n",
        "            compression_ratio = compressed_size / original_size if original_size != 0 else None\n",
        "        else:\n",
        "            compression_ratio = None\n",
        "\n",
        "        # Adicionar dados à lista de compressão\n",
        "        compression_data.append({\n",
        "            'File_Name': filename,\n",
        "            'File_Type': ftype,\n",
        "            'Compression_Ratio': compression_ratio,\n",
        "            'Dictionary_Size': compress_stats.get('dictionary_size_over_time', [])[-1] if compress_stats.get('dictionary_size_over_time') else None,\n",
        "            'Compression_Time(s)': compress_stats.get('execution_time', 0),\n",
        "            'Memory_Usage(KB)': compress_stats.get('memory_usage', 0)\n",
        "        })\n",
        "\n",
        "        # Adicionar dados à lista de descompressão\n",
        "        decompression_data.append({\n",
        "            'File_Name': filename,\n",
        "            'File_Type': ftype,\n",
        "            'Decompression_Time(s)': decompress_stats.get('execution_time', 0)\n",
        "        })\n",
        "\n",
        "        # Armazenar estatísticas detalhadas para o arquivo\n",
        "        detailed_stats[filename] = {\n",
        "            'compression_ratio_over_time': compress_stats.get('compression_ratio_over_time', []),\n",
        "            'dictionary_size_over_time': compress_stats.get('dictionary_size_over_time', []),\n",
        "            'memory_usage': compress_stats.get('memory_usage', 0)\n",
        "        }\n",
        "\n",
        "    else:\n",
        "        print(f\"Arquivo de estatísticas de processamento não encontrado: {process_stats_file}\")\n",
        "\n",
        "# Criar DataFrames\n",
        "compression_df = pd.DataFrame(compression_data)\n",
        "decompression_df = pd.DataFrame(decompression_data)\n",
        "\n",
        "# Exibir DataFrames\n",
        "print(\"Dados de Compressão:\")\n",
        "display(compression_df)\n",
        "\n",
        "print(\"Dados de Descompressão:\")\n",
        "display(decompression_df)\n",
        "\n",
        "# Função para determinar limites globais para escalas consistentes\n",
        "def get_global_limits(df, column):\n",
        "    min_val = df[column].min()\n",
        "    max_val = df[column].max()\n",
        "    return min_val, max_val\n",
        "\n",
        "# Obter limites globais para cada métrica\n",
        "compression_time_min, compression_time_max = get_global_limits(compression_df, 'Compression_Time(s)') if not compression_df.empty else (0, 1)\n",
        "decompression_time_min, decompression_time_max = get_global_limits(decompression_df, 'Decompression_Time(s)') if not decompression_df.empty else (0, 1)\n",
        "compression_ratio_min, compression_ratio_max = get_global_limits(compression_df, 'Compression_Ratio') if not compression_df.empty else (0, 1)\n",
        "memory_usage_min, memory_usage_max = get_global_limits(compression_df, 'Memory_Usage(KB)') if not compression_df.empty else (0, 1)\n",
        "\n",
        "# Função para ordenar o DataFrame e retornar a ordem dos arquivos\n",
        "def get_sorted_order(df, y_column):\n",
        "    sorted_df = df.sort_values(by=y_column)\n",
        "    return sorted_df['File_Name']\n",
        "\n",
        "# 1. Gráfico de Tempo de Compressão e Descompressão Ordenados\n",
        "fig, axes = plt.subplots(1, 2, figsize=(24, 10))\n",
        "\n",
        "# Tempo de Compressão\n",
        "if not compression_df.empty:\n",
        "    order = get_sorted_order(compression_df, 'Compression_Time(s)')\n",
        "    sns.barplot(ax=axes[0], x='File_Name', y='Compression_Time(s)', hue='File_Type', data=compression_df, order=order)\n",
        "    axes[0].set_title('Tempo de Compressão por Arquivo')\n",
        "    axes[0].set_xlabel('Arquivo')\n",
        "    axes[0].set_ylabel('Tempo de Compressão (segundos)')\n",
        "    axes[0].set_ylim(compression_time_min, compression_time_max * 1.1)  # Adiciona 10% para visualização\n",
        "    axes[0].tick_params(axis='x', rotation=45)\n",
        "else:\n",
        "    axes[0].text(0.5, 0.5, 'Nenhum dado de compressão disponível para plotar.', horizontalalignment='center', verticalalignment='center')\n",
        "    axes[0].set_axis_off()\n",
        "\n",
        "# Tempo de Descompressão\n",
        "if not decompression_df.empty:\n",
        "    order = get_sorted_order(decompression_df, 'Decompression_Time(s)')\n",
        "    sns.barplot(ax=axes[1], x='File_Name', y='Decompression_Time(s)', hue='File_Type', data=decompression_df, order=order)\n",
        "    axes[1].set_title('Tempo de Descompressão por Arquivo')\n",
        "    axes[1].set_xlabel('Arquivo')\n",
        "    axes[1].set_ylabel('Tempo de Descompressão (segundos)')\n",
        "    axes[1].set_ylim(decompression_time_min, decompression_time_max * 1.1)\n",
        "    axes[1].tick_params(axis='x', rotation=45)\n",
        "else:\n",
        "    axes[1].text(0.5, 0.5, 'Nenhum dado de descompressão disponível para plotar.', horizontalalignment='center', verticalalignment='center')\n",
        "    axes[1].set_axis_off()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2. Gráfico de Taxa de Compressão e Uso de Memória Ordenados\n",
        "fig, axes = plt.subplots(1, 2, figsize=(24, 10))\n",
        "\n",
        "# Taxa de Compressão\n",
        "if not compression_df.empty:\n",
        "    order = get_sorted_order(compression_df, 'Compression_Ratio')\n",
        "    sns.barplot(ax=axes[0], x='File_Name', y='Compression_Ratio', hue='File_Type', data=compression_df, order=order)\n",
        "    axes[0].set_title('Taxa de Compressão por Arquivo')\n",
        "    axes[0].set_xlabel('Arquivo')\n",
        "    axes[0].set_ylabel('Taxa de Compressão (Comprimido / Original)')\n",
        "    axes[0].set_ylim(compression_ratio_min * 0.9, compression_ratio_max * 1.1)\n",
        "    axes[0].tick_params(axis='x', rotation=45)\n",
        "else:\n",
        "    axes[0].text(0.5, 0.5, 'Nenhum dado de compressão disponível para plotar.', horizontalalignment='center', verticalalignment='center')\n",
        "    axes[0].set_axis_off()\n",
        "\n",
        "# Uso de Memória\n",
        "if not compression_df.empty:\n",
        "    order = get_sorted_order(compression_df, 'Memory_Usage(KB)')\n",
        "    sns.barplot(ax=axes[1], x='File_Name', y='Memory_Usage(KB)', hue='File_Type', data=compression_df, order=order)\n",
        "    axes[1].set_title('Uso de Memória do Dicionário por Arquivo')\n",
        "    axes[1].set_xlabel('Arquivo')\n",
        "    axes[1].set_ylabel('Uso de Memória (KB)')\n",
        "    axes[1].set_ylim(memory_usage_min * 0.9, memory_usage_max * 1.1)\n",
        "    axes[1].tick_params(axis='x', rotation=45)\n",
        "else:\n",
        "    axes[1].text(0.5, 0.5, 'Nenhum dado de uso de memória disponível para plotar.', horizontalalignment='center', verticalalignment='center')\n",
        "    axes[1].set_axis_off()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 3. Gráficos Detalhados por Arquivo\n",
        "\n",
        "# Preparar limites globais para os gráficos de Taxa de Compressão e Tamanho do Dicionário\n",
        "global_compression_ratio_max = max([max(stats['compression_ratio_over_time']) if stats['compression_ratio_over_time'] else 0 for stats in detailed_stats.values()], default=1)\n",
        "global_dictionary_size_max = max([max(stats['dictionary_size_over_time']) if stats['dictionary_size_over_time'] else 0 for stats in detailed_stats.values()], default=1)\n",
        "\n",
        "# Plotar Taxa de Compressão ao Longo do Tempo para cada arquivo\n",
        "for filename in files_to_process:\n",
        "    if filename in detailed_stats:\n",
        "        compression_ratio_over_time = detailed_stats[filename]['compression_ratio_over_time']\n",
        "        if compression_ratio_over_time:\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            plt.plot(compression_ratio_over_time, marker='o')\n",
        "            plt.title(f'Taxa de Compressão ao Longo do Tempo - {filename}')\n",
        "            plt.xlabel('Iteração')\n",
        "            plt.ylabel('Taxa de Compressão')\n",
        "            plt.ylim(0, global_compression_ratio_max * 1.1)\n",
        "            plt.grid(True)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(f\"Sem dados de taxa de compressão ao longo do tempo para {filename}\")\n",
        "    else:\n",
        "        print(f\"Estatísticas detalhadas não encontradas para {filename}\")\n",
        "\n",
        "# Plotar Tamanho do Dicionário ao Longo do Tempo para cada arquivo\n",
        "for filename in files_to_process:\n",
        "    if filename in detailed_stats:\n",
        "        dictionary_size_over_time = detailed_stats[filename]['dictionary_size_over_time']\n",
        "        if dictionary_size_over_time:\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            plt.plot(dictionary_size_over_time, color='orange', marker='o')\n",
        "            plt.title(f'Tamanho do Dicionário ao Longo do Tempo - {filename}')\n",
        "            plt.xlabel('Iteração')\n",
        "            plt.ylabel('Tamanho do Dicionário')\n",
        "            plt.ylim(0, global_dictionary_size_max * 1.1)\n",
        "            plt.grid(True)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(f\"Sem dados de tamanho do dicionário ao longo do tempo para {filename}\")\n",
        "    else:\n",
        "        print(f\"Estatísticas detalhadas não encontradas para {filename}\")\n",
        "\n",
        "# 4. Gráfico Comparativo do Uso de Memória\n",
        "\n",
        "# Plotar Uso de Memória do Dicionário por Arquivo Ordenados\n",
        "if not compression_df.empty:\n",
        "    # Ordenar os arquivos do menor para o maior uso de memória\n",
        "    compression_df_sorted_memory = compression_df.sort_values(by='Memory_Usage(KB)')\n",
        "    order = compression_df_sorted_memory['File_Name']\n",
        "\n",
        "    plt.figure(figsize=(18, 10))\n",
        "    sns.barplot(x='File_Name', y='Memory_Usage(KB)', hue='File_Type', data=compression_df, order=order)\n",
        "    plt.title('Uso de Memória do Dicionário por Arquivo')\n",
        "    plt.xlabel('Arquivo')\n",
        "    plt.ylabel('Uso de Memória (KB)')\n",
        "    plt.ylim(memory_usage_min * 0.9, memory_usage_max * 1.1)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.legend(title='Tipo de Arquivo')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Nenhum dado de uso de memória disponível para plotar.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1ucHQAriBmF",
        "outputId": "ea874f27-d6b5-4ef6-b82a-03bfe911e1c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Diretório 'compressed_test_cases' criado para armazenar os arquivos comprimidos.\n",
            "Arquivo com extensão não suportada encontrado e será ignorado: notebook_1mb.tiff\n",
            "Arquivo com extensão não suportada encontrado e será ignorado: lorem_5kb.docx\n",
            "Arquivo com extensão não suportada encontrado e será ignorado: .DS_Store\n",
            "Processando test_cases/imagemhomog_30kb.bmp...\n",
            "Compressão...\n",
            "Arquivo comprimido criado: compressed_test_cases/imagemhomog_30kb.bin\n",
            "Estatísticas de compressão salvas em 'test_cases/imagemhomog_30kb.bmp_process_stats.json'\n",
            "Descompressão...\n",
            "Descompressão concluída: decompressed_imagemhomog_30kb_decompressed.bmp\n",
            "Estatísticas de descompressão salvas em 'test_cases/imagemhomog_30kb.bmp_process_stats.json'\n",
            "Processo concluído.\n",
            "\n",
            "Processando test_cases/StarWars3_132kb.wav...\n",
            "Compressão...\n",
            "Arquivo comprimido criado: compressed_test_cases/StarWars3_132kb.bin\n",
            "Estatísticas de compressão salvas em 'test_cases/StarWars3_132kb.wav_process_stats.json'\n",
            "Descompressão...\n",
            "Tipo de arquivo não suportado!\n",
            "Descompressão concluída: decompressed_StarWars3_132kb_decompressed.wav\n",
            "Estatísticas de descompressão salvas em 'test_cases/StarWars3_132kb.wav_process_stats.json'\n",
            "Processo concluído.\n",
            "\n",
            "Processando test_cases/lorem_5kb.txt...\n",
            "Compressão...\n",
            "Arquivo comprimido criado: compressed_test_cases/lorem_5kb.bin\n",
            "Estatísticas de compressão salvas em 'test_cases/lorem_5kb.txt_process_stats.json'\n",
            "Descompressão...\n",
            "Descompressão concluída: decompressed_lorem_5kb_decompressed.txt\n",
            "Estatísticas de descompressão salvas em 'test_cases/lorem_5kb.txt_process_stats.json'\n",
            "Processo concluído.\n",
            "\n",
            "Processando test_cases/smallbin_10kb.bin...\n",
            "Compressão...\n",
            "Arquivo comprimido criado: compressed_test_cases/smallbin_10kb.bin\n",
            "Estatísticas de compressão salvas em 'test_cases/smallbin_10kb.bin_process_stats.json'\n",
            "Descompressão...\n",
            "Tipo de arquivo não suportado!\n",
            "Descompressão concluída: decompressed_smallbin_10kb_decompressed.bin\n",
            "Estatísticas de descompressão salvas em 'test_cases/smallbin_10kb.bin_process_stats.json'\n",
            "Processo concluído.\n",
            "\n",
            "Processando test_cases/textorepetido_27kb.txt...\n",
            "Compressão...\n",
            "Arquivo comprimido criado: compressed_test_cases/textorepetido_27kb.bin\n",
            "Estatísticas de compressão salvas em 'test_cases/textorepetido_27kb.txt_process_stats.json'\n",
            "Descompressão...\n",
            "Descompressão concluída: decompressed_textorepetido_27kb_decompressed.txt\n",
            "Estatísticas de descompressão salvas em 'test_cases/textorepetido_27kb.txt_process_stats.json'\n",
            "Processo concluído.\n",
            "\n",
            "Processando test_cases/CantinaBand3_132kb.wav...\n",
            "Compressão...\n",
            "Arquivo comprimido criado: compressed_test_cases/CantinaBand3_132kb.bin\n",
            "Estatísticas de compressão salvas em 'test_cases/CantinaBand3_132kb.wav_process_stats.json'\n",
            "Descompressão...\n",
            "Tipo de arquivo não suportado!\n",
            "Descompressão concluída: decompressed_CantinaBand3_132kb_decompressed.wav\n",
            "Estatísticas de descompressão salvas em 'test_cases/CantinaBand3_132kb.wav_process_stats.json'\n",
            "Processo concluído.\n",
            "\n",
            "Processando test_cases/casa_1mb.bmp...\n"
          ]
        }
      ]
    }
  ]
}